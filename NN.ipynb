{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time, json, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist_loader\n",
    "\n",
    "trained_model_file = './TrainedModelData.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "\t\"\"\"\n",
    "\tSGD (short for Stochastic Gradient Descent), represents an optimization algorithm \n",
    "\tthat can be used to train a neural network model.\n",
    "\tThe __call__ method of the SGD class performs a single iteration of training, \n",
    "\twhich includes iterating over the training data in mini-batches, \n",
    "\tmaking predictions using the forward propagation method, and updating the model's weights and biases \n",
    "\tusing the back-propagation method. \n",
    "\tThe forward_propagation and back_propagation methods are helper methods that implement the \n",
    "\tforward and backward passes of the training algorithm, respectively. \n",
    "\tThe SGD class is initialized with a reference to the MLP model that it will be optimizing.\n",
    "\t\"\"\"\n",
    "\tdef __init__( self, model = None ):\n",
    "\t\tself.model         = model\n",
    "\t\tself.learning_rate = model.learning_rate\n",
    "\t\tself.lmbda         = model.lmbda\n",
    "\t\tself.loss          = model.loss\n",
    "\t\tself.activation    = model.activation\n",
    "\t\tself.num_layers    = model.num_layers\n",
    "\n",
    "\tdef __call__( self, batch, num_data ):\n",
    "\t\tnabla_W = [ np.zeros( w.shape ) for w in self.model.weights ]\n",
    "\t\tnabla_B = [ np.zeros( b.shape ) for b in self.model.biases ]\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tThis loop calculates the gradient of loss with respect to all variables, \n",
    "\t\tsums them up and stores in nabla_Wm nabla_B lists,\n",
    "\t\t\"\"\"\n",
    "\t\tfor inputs, labels in batch:\n",
    "\t\t\ta  = inputs \t# Activation\n",
    "\t\t\tAs = [inputs] \t# list to store all the Activation matrix, layer by layer\n",
    "\t\t\tZs = []\t\t\t# list to store all the z vectors, layer by layer\n",
    "\n",
    "\t\t\tself.forward_propagation( a, As, Zs )\n",
    "\t\t\tdelta_Nabla_W, delta_Nabla_B = self.back_propagation(As, Zs, labels)\n",
    "\n",
    "\t\t\t# Summing up all the changes in weights amd biases\n",
    "\t\t\tnabla_W = [ nw + d_nw for nw, d_nw in zip( nabla_W, delta_Nabla_W ) ]\n",
    "\t\t\tnabla_B = [ nb + d_nb for nb, d_nb in zip( nabla_B, delta_Nabla_B ) ]\n",
    "\n",
    "\t\t# Taking steps towards the lowest loss function\n",
    "\t\t# lmbda, regularization parameter\n",
    "\t\tregularization  = 1 - ( self.learning_rate * ( self.lmbda / num_data ) )\n",
    "\t\tnw_coef = self.learning_rate / len(batch)\n",
    "\n",
    "\t\tself.model.weights = [\n",
    "\t\t\t( regularization * w ) - ( nw_coef * nw )\n",
    "\t\t\tfor w, nw in zip( self.model.weights, nabla_W )\n",
    "\t\t]\n",
    "\n",
    "\t\tself.model.biases = [ \n",
    "\t\t\tb - ( self.learning_rate / len(batch) ) * nb\n",
    "\t\t\tfor b, nb in zip( self.model.biases, nabla_B ) \n",
    "\t\t]\n",
    "\t\n",
    "\tdef forward_propagation( self, a, As=None, Zs=None  ):\n",
    "\t\t\"\"\"\n",
    "\t\tThis code defines a forward_propagation algorithm. \n",
    "\t\tIt is called during the training of out neural network.\n",
    "\n",
    "\t\tThe method takes three arguments:\n",
    "\n",
    "\t\ta  : a matrix of input activations\n",
    "\t\tAs : a list to store the activation matrices, layer by layer\n",
    "\t\tZs : a list to store the z vectors, layer by layer\n",
    "\n",
    "\t\tThe method uses the weights and biases of the model to perform a \n",
    "\t\tforward propagation through the network. \n",
    "\t\tThis means that the input activations \"a\" are passed through the network layer by layer, \n",
    "\t\tand the output activations are returned by the method.\n",
    "\n",
    "\t\tIn addition to returning the output activations, \n",
    "\t\tthe method also appends the intermediate activation matrices \n",
    "\t\tand z vectors to the As and Zs lists if they are provided. \n",
    "\t\tThese lists will be used later in the back-propagation step of training.\n",
    "\t\t\"\"\"\n",
    "\t\tfor W, b in zip( self.model.weights, self.model.biases ):\n",
    "\t\t\tz = np.dot( W, a ) + b\n",
    "\t\t\tif Zs != None : Zs.append(z)\n",
    "\n",
    "\t\t\ta = self.activation(z)\n",
    "\t\t\tif As != None : As.append(a)\n",
    "\t\treturn a\n",
    "\t\n",
    "\tdef back_propagation( self, As, Zs, labels ):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function is implementing back-propagation, a method for training a neural network. \n",
    "\t\tThe back-propagation algorithm is used to calculate the gradient of the loss function with respect to \n",
    "\t\tthe weights and biases of the network. This is used to update the weights and biases to reduce the loss \n",
    "\t\tand improve the performance of the network on a given task.\n",
    "\n",
    "\t\tIt takes three arguments: \n",
    "\t\tAs\t\t : a list to store the activation matrices, layer by layer\n",
    "\t\tZs\t\t : a list to store the z vectors, layer by layer , where z = (a * w) + b\n",
    "\t\tlabels : a numpy array of target values for the network.\n",
    "\n",
    "\t\tFirst the method initializes two lists, nabla_W and nabla_B, to store the gradients of the loss function \n",
    "\t\twith respect to the weights and biases of the network, respectively. \n",
    "\t\tThese are initialized to be arrays of zeros with the same shape as the weight and bias arrays of the network.\n",
    "\n",
    "\t\tNext, it calculates the gradient of the loss with respect to the output of the network, \n",
    "\t\tand stores the result in delta_error. It then uses this to calculate the gradient of the loss with respect to \n",
    "\t\tthe final layer's weights and biases, and stores these in nabla_W and nabla_B, respectively.\n",
    "\n",
    "\t\tIt then loops through the layers of the network in reverse order (from the second-to-last layer to the first layer), \n",
    "\t\tand uses the previously calculated gradients to compute the gradients of the loss with respect to the weights and biases \n",
    "\t\tof each layer.\n",
    "\n",
    "\t\tFinally, the method returns the calculated gradients in nabla_W and nabla_B.\n",
    "\t\t\"\"\"\n",
    "\t\tnabla_W = [ np.zeros( w.shape ) for w in self.model.weights ]\n",
    "\t\tnabla_B = [ np.zeros( b.shape ) for b in self.model.biases ]\n",
    "\n",
    "\t\tdelta_error = self.loss.delta( As[-1], labels )\n",
    "\t\tnabla_W[-1] = np.dot( delta_error, As[-2].transpose() )\n",
    "\t\tnabla_B[-1] = delta_error\n",
    "\t\n",
    "\t\t# Backwards pass / Back-propagation\n",
    "\t\tfor l in range(2, self.num_layers):\n",
    "\t\t\tdelta_error = np.dot( self.model.weights[-l + 1].transpose(), delta_error ) * self.activation.prime( Zs[-l] )\n",
    "\t\t\tnabla_W[-l] = np.dot( delta_error, As[-l - 1].transpose() )\n",
    "\t\t\tnabla_B[-l] = delta_error\n",
    "\n",
    "\t\treturn nabla_W, nabla_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef forward_propagation( self, a, As=None, Zs=None  ):\n",
    "\t\t\"\"\"\n",
    "\t\tThis code defines a forward_propagation algorithm. \n",
    "\t\tIt is called during the training of out neural network.\n",
    "\n",
    "\t\tThe method takes three arguments:\n",
    "\n",
    "\t\ta  : a matrix of input activations\n",
    "\t\tAs : a list to store the activation matrices, layer by layer\n",
    "\t\tZs : a list to store the z vectors, layer by layer\n",
    "\n",
    "\t\tThe method uses the weights and biases of the model to perform a \n",
    "\t\tforward propagation through the network. \n",
    "\t\tThis means that the input activations \"a\" are passed through the network layer by layer, \n",
    "\t\tand the output activations are returned by the method.\n",
    "\n",
    "\t\tIn addition to returning the output activations, \n",
    "\t\tthe method also appends the intermediate activation matrices \n",
    "\t\tand z vectors to the As and Zs lists if they are provided. \n",
    "\t\tThese lists will be used later in the back-propagation step of training.\n",
    "\t\t\"\"\"\n",
    "\t\tfor W, b in zip( self.model.weights, self.model.biases ):\n",
    "\t\t\tz = np.dot( W, a ) + b\n",
    "\t\t\tif Zs != None : Zs.append(z)\n",
    "\n",
    "\t\t\ta = self.activation(z)\n",
    "\t\t\tif As != None : As.append(a)\n",
    "\t\treturn a\n",
    "\t\n",
    "\tdef back_propagation( self, As, Zs, labels ):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function is implementing back-propagation, a method for training a neural network. \n",
    "\t\tThe back-propagation algorithm is used to calculate the gradient of the loss function with respect to \n",
    "\t\tthe weights and biases of the network. This is used to update the weights and biases to reduce the loss \n",
    "\t\tand improve the performance of the network on a given task.\n",
    "\n",
    "\t\tIt takes three arguments: \n",
    "\t\tAs\t\t : a list to store the activation matrices, layer by layer\n",
    "\t\tZs\t\t : a list to store the z vectors, layer by layer , where z = (a * w) + b\n",
    "\t\tlabels : a numpy array of target values for the network.\n",
    "\n",
    "\t\tFirst the method initializes two lists, nabla_W and nabla_B, to store the gradients of the loss function \n",
    "\t\twith respect to the weights and biases of the network, respectively. \n",
    "\t\tThese are initialized to be arrays of zeros with the same shape as the weight and bias arrays of the network.\n",
    "\n",
    "\t\tNext, it calculates the gradient of the loss with respect to the output of the network, \n",
    "\t\tand stores the result in delta_error. It then uses this to calculate the gradient of the loss with respect to \n",
    "\t\tthe final layer's weights and biases, and stores these in nabla_W and nabla_B, respectively.\n",
    "\n",
    "\t\tIt then loops through the layers of the network in reverse order (from the second-to-last layer to the first layer), \n",
    "\t\tand uses the previously calculated gradients to compute the gradients of the loss with respect to the weights and biases \n",
    "\t\tof each layer.\n",
    "\n",
    "\t\tFinally, the method returns the calculated gradients in nabla_W and nabla_B.\n",
    "\t\t\"\"\"\n",
    "\t\tnabla_W = [ np.zeros( w.shape ) for w in self.model.weights ]\n",
    "\t\tnabla_B = [ np.zeros( b.shape ) for b in self.model.biases ]\n",
    "\n",
    "\t\tdelta_error = self.loss.delta( As[-1], labels )\n",
    "\t\tnabla_W[-1] = np.dot( delta_error, As[-2].transpose() )\n",
    "\t\tnabla_B[-1] = delta_error\n",
    "\t\n",
    "\t\t# Backwards pass / Back-propagation\n",
    "\t\tfor l in range(2, self.num_layers):\n",
    "\t\t\tdelta_error = np.dot( self.model.weights[-l + 1].transpose(), delta_error ) * self.activation.prime( Zs[-l] )\n",
    "\t\t\tnabla_W[-l] = np.dot( delta_error, As[-l - 1].transpose() )\n",
    "\t\t\tnabla_B[-l] = delta_error\n",
    "\n",
    "\t\treturn nabla_W, nabla_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "\t\"\"\"\n",
    "\tCrossEntropyLoss implements the cross-entropy loss function for a binary classification task.\n",
    "\tIt measures the difference between the predicted probability distribution over classes and \n",
    "\tthe true probability distribution over classes.\n",
    "\t\"\"\"\n",
    "\t@staticmethod\n",
    "\tdef func(a, y):\n",
    "\t\t\"\"\"\n",
    "\t\tThe func method takes two arguments, a and y, and calculates the cross-entropy loss for a given prediction a \n",
    "\t\tand true label y. This is done by summing the cross-entropy loss for each example in the batch of data.\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.sum( np.nan_to_num( -y * np.log(a) - (1-y) * np.log(1-a) ) )\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef delta(a, y):\n",
    "\t\t\"\"\"\n",
    "\t\tThe delta method calculates the gradient of the cross-entropy loss with respect to the output of the network. \n",
    "\t\tIt simply returns the difference between the predicted probabilities (a) and the true labels (y). \n",
    "\t\tThis is used in the back-propagation algorithm to calculate the gradient of the loss with respect to the \n",
    "\t\tweights and biases of the network.\n",
    "\t\t\"\"\"\n",
    "\t\treturn ( a - y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "\t\"\"\"\n",
    "\tThe sigmoid function is often used as the activation function for neurons in a neural network. \n",
    "\tIt maps a real-valued input to a value between 0 and 1, which can be interpreted as a probability.\n",
    "\t\"\"\"\n",
    "\tdef __call__( self, z ):\n",
    "\t\t\"\"\"\n",
    "\t\tThis method is called when an instance of the class is called as a function, \n",
    "\t\tand it calculates the sigmoid function for the given input z.\n",
    "\t\t\"\"\"\n",
    "\t\treturn 1.0 / ( 1.0 + np.exp(-z) )\n",
    "\n",
    "\tdef prime( self, z ):\n",
    "\t\t\"\"\"\n",
    "\t\tThe prime method calculates the derivative of the sigmoid function for the given input z. \n",
    "\t\tThis is used in the back-propagation algorithm to calculate the gradient of the loss with respect to the inputs to each neuron in the network.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self( z ) * ( 1.0 - self( z ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP( object ):\n",
    "\t\"\"\"\n",
    "\tA basic Multi-Layers Perceptron model, which represents a basic neural network model. \n",
    "\tThe class constructor ( __init__ ) takes several parameters that specify the model's configuration, \n",
    "\tsuch as the number of layers in the network, the type of optimizer and loss function to use, and the learning rate. \n",
    "\tThe fit method trains the model on a given dataset, while the predict method uses the trained model \n",
    "\tto make inference on new data.\n",
    "\tThe MLP class also includes several helper methods such as __make_batches which divides the training data \n",
    "\tinto batches, and __load_variables which loads previously saved weights and biases for the model.\n",
    "\t\"\"\"\n",
    "\tdef __init__( \n",
    "\t\t\tself,\n",
    "\t\t\tlayers        = [784,30,10],\n",
    "\t\t\toptimizer     = SGD,\n",
    "\t\t\tloss          = CrossEntropyLoss,\n",
    "\t\t\tactivation    = Sigmoid,\n",
    "\t\t\tlearning_rate = 0.1,\n",
    "\t\t\tlmbda         = 5.0,\n",
    "\t\t):\n",
    "\t\tself.num_layers    = len(layers)\n",
    "\t\tself.layers        = layers\n",
    "\t\tself.loss          = loss()\n",
    "\t\tself.activation    = activation()\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.lmbda         = lmbda\n",
    "\t\tself.optimizer     = optimizer( model=self )\n",
    "\t\tself.weights       = [ \n",
    "\t\t\tnp.random.randn(y, x) / np.sqrt(x)\n",
    "\t\t\tfor x, y in zip( layers[:-1], layers[1:] )\n",
    "\t\t]\n",
    "\t\tself.biases\t= [ np.random.randn(y, 1) for y in layers[1:] ]\n",
    "\t\tself.is_last_epoch = False\n",
    "\t\tself.num_train_data = 0\n",
    "\t\tself.epoch_start_time = 0\n",
    "\t\n",
    "\tdef fit(\n",
    "\t\t\tself, \n",
    "\t\t\ttrain_data     = None,\n",
    "\t\t\tval_data       = None,\n",
    "\t\t\tepochs         = 10,\n",
    "\t\t\tbatch_size     = 32,\n",
    "\t\t\tcontinue_train = False,\n",
    "\t\t\tmonitor        = None\n",
    "\t\t):\n",
    "\t\tself.num_train_data = len( train_data )\n",
    "\n",
    "\t\tif ( continue_train ):\n",
    "\t\t\tself.__load_viriables()\n",
    "\t\n",
    "\t\tepochs_progress_bar = tqdm( range(epochs), ncols=85, position=0 )\n",
    "\t\tfor epoch in epochs_progress_bar:\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tThis is the training loop for our neural network. \n",
    "\t\t\tIt iterates through a number of epochs (passes over the training data) and within each epoch, \n",
    "\t\t\tit splits the training data into batches and trains the model on each batch using the specified optimizer.\n",
    "\t\t\tFor each epoch, it also calls the monitor function to record the training and validation metrics.\n",
    "\t\t\tThe epochs_progress_bar and batches_progress_bar are using the tqdm library to show progress bars for the training loop.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tself.epoch_start_time   = time.time()\n",
    "\t\t\tself.is_last_epoch = True if epoch+1 == epochs else False\n",
    "\t\t\tepochs_progress_bar.set_description( f'Epoch {epoch+1}' )\n",
    "\t\t\tbatches = self.__make_batches( train_data, batch_size )\n",
    "\n",
    "\t\t\tbatches_progress_bar = tqdm(batches, desc=f'Batch', ncols=75, position=1, ascii=False, leave=False)\n",
    "\t\t\tfor batch in batches_progress_bar:\n",
    "\t\t\t\tself.optimizer( batch, self.num_train_data )\n",
    "\n",
    "\t\t\tif ( monitor != None ):\n",
    "\t\t\t\tmonitor( self, epoch, self.epoch_start_time, self.num_train_data, train_data, val_data )\n",
    "\t\t\t\n",
    "\tdef __load_viriables(self):\n",
    "\t\twith open(trained_model_file, \"r\") as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tself.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "\t\tself.biases  = [np.array(b) for b in data[\"biases\"]]\n",
    "\t\tprint( \"Last saved, Hidden neurons: \", len( self.weights[0] ), \", \", data[\"accuracy\"] )\n",
    "\t\n",
    "\tdef __make_batches( self, data, batch_size ):\n",
    "\t\tnp.random.shuffle( data )\n",
    "\t\t# Creating batches out of training data\n",
    "\t\tbatches = [\n",
    "\t\t\tdata[ k : k + batch_size ]\n",
    "\t\t\tfor k in range( 0, self.num_train_data, batch_size )\n",
    "\t\t]\n",
    "\t\treturn batches\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\ta = self.optimizer.forward_propagation( x )\n",
    "\t\treturn ( np.argmax(a) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monitor():\n",
    "\t\"\"\"\n",
    "\tThis class monitors the performance of the model. \n",
    "\tIt has attributes for tracking whether training and validation data should be monitored, \n",
    "\tas well as whether the current model parameters should be saved if they yield the best performance so far. \n",
    "\tThe Monitor class also has methods for computing the total loss and accuracy on a given dataset, \n",
    "\tand for printing the current values of these metrics. \n",
    "\tWhen the __call__ method of the Monitor instance is invoked, it updates the relevant metrics and prints them.\n",
    "\t\"\"\"\n",
    "\tdef __init__( self, training=False, validation=True, save=False ):\n",
    "\t\tself.training   = training\n",
    "\t\tself.validation = validation\n",
    "\t\tself.save       = save\n",
    "\n",
    "\t\tself.evaluation_loss, self.evaluation_accuracy = [], []\n",
    "\t\tself.training_loss, self.training_accuracy = [], []\n",
    "\t\tself.maxAccuracy = 0;\n",
    "\t\tself.max_w       = []\n",
    "\t\tself.max_b       = []\n",
    "\t\tself.train_loss, self.train_accuracy, self.val_loss, self.val_accuracy = 0,0,0,0\n",
    "\t\n",
    "\tdef __call__( self, model, epoch, epoch_start_time, num_train_data, train_data, val_data ):\n",
    "\t\tself.model = model\n",
    "\t\tself.is_val_data = val_data != None\n",
    "\t\t\n",
    "\t\tif self.training:\n",
    "\t\t\tself.train_loss = self.total_loss(train_data)\n",
    "\t\t\tself.training_loss.append(self.train_loss)\n",
    "\t\n",
    "\t\t\tself.train_accuracy = self.accuracy(train_data, convert=True)\n",
    "\t\t\tself.training_accuracy.append(self.train_accuracy)\n",
    "\n",
    "\t\tif self.is_val_data & self.validation:\n",
    "\t\t\tself.num_val_data = len( val_data )\n",
    "\t\t\tself.val_loss = self.total_loss(val_data, convert=True)\n",
    "\t\t\tself.evaluation_loss.append(self.val_loss)\n",
    "\n",
    "\t\t\tself.val_accuracy = self.accuracy(val_data)\n",
    "\t\t\tself.evaluation_accuracy.append(self.val_accuracy)\n",
    "\t\t\n",
    "\t\tself.print_metrics( epoch, epoch_start_time, num_train_data )\n",
    "\n",
    "\tdef print_metrics( self, epoch, epoch_start_time, num_train_data ):\n",
    "\t\t\"\"\"\n",
    "\t\tPrints the relevant training metrics to the console.\n",
    "\t\t\"\"\"\n",
    "\t\tloss       = round( self.train_loss, 2 ) if self.training  else 0\n",
    "\t\taccuracy   = round( ( self.train_accuracy/num_train_data ), 2 ) if self.training  else 0\n",
    "\t\tv_loss     = round( self.val_loss, 2 ) if self.is_val_data & self.validation  else 0\n",
    "\t\tv_accuracy = round( ( self.val_accuracy/self.num_val_data ), 2 ) if self.is_val_data & self.validation  else 0\n",
    "\t\tepoch_time = str( round( time.time() - epoch_start_time, 2 ) ) + \"s\"\n",
    "\t\tif self.training & self.is_val_data & self.validation:\n",
    "\t\t\ttqdm.write( \n",
    "\t\t\t\tf'{str(epoch+1):3s}: {\"loss\"}: {str(loss):4s} — {\"accuracy\"}: {str(accuracy):4s} — {\"val_loss\"}: {str(v_loss):4s} — {\"val_accuracy\"}: {str(v_accuracy):4s} — {epoch_time}' \n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\telif self.training & ( (self.is_val_data == False) or (self.validation == False) ):\n",
    "\t\t\ttqdm.write( \n",
    "\t\t\t\tf'{str(epoch+1):3s}: {\"loss\"}: {str(loss):4s} — {\"accuracy\"}: {str(accuracy):4s} — {epoch_time}' \n",
    "\t\t\t)\n",
    "\n",
    "\t\telif self.is_val_data & self.validation & (self.training == False):\n",
    "\t\t\ttqdm.write( \n",
    "\t\t\t\tf'{str(epoch+1):3s}: {\"val_loss\"}: {str(v_loss):4s} — {\"val_accuracy\"}: {str(v_accuracy):4s} — {epoch_time}' \n",
    "\t\t\t)\n",
    "\n",
    "\t\tif ( self.val_accuracy > self.maxAccuracy ):\n",
    "\t\t\tself.maxAccuracy = self.val_accuracy\n",
    "\t\t\tself.max_w, self.max_b = self.model.weights, self.model.biases\n",
    "\n",
    "\t\tif ( self.save ): \n",
    "\t\t\tself.save_model()\n",
    "\t\t\tif ( self.model.is_last_epoch ):\n",
    "\t\t\t\tself.model.weights, self.model.biases = self.max_w, self.max_b\n",
    "\t\t\t\tself.save_model()\n",
    "\n",
    "\tdef save_model( self ):\n",
    "\t\t\"\"\"\n",
    "\t\tSaves the model variables.\n",
    "\t\t\"\"\"\n",
    "\t\tdata = {\n",
    "\t\t\t\"layers\"       : self.model.layers,\n",
    "\t\t\t\"weights\"      : [ w.tolist() for w in self.model.weights ],\n",
    "\t\t\t\"biases\"       : [ b.tolist() for b in self.model.biases ],\n",
    "\t\t\t\"optimizer\"    : str( self.model.optimizer.__class__.__name__ ),\n",
    "\t\t\t\"loss\"         : str( self.model.loss.__class__.__name__ ),\n",
    "\t\t\t\"activation\"   : str( self.model.activation.__class__.__name__ ),\n",
    "\t\t\t\"learning_rate\": self.model.learning_rate,\n",
    "\t\t\t\"lmbda\"        : self.model.lmbda,\n",
    "\t\t\t\"accuracy\"     : self.maxAccuracy,\n",
    "\t\t}\n",
    "\t\twith open(trained_model_file, \"w\") as f:\n",
    "\t\t\tjson.dump(data, f)\n",
    "\n",
    "\tdef history(self):\n",
    "\t\treturn self.evaluation_loss, self.evaluation_accuracy, \\\n",
    "\t\t\tself.training_loss, self.training_accuracy\n",
    "\n",
    "\tdef accuracy(self, data, convert=False):\n",
    "\t\t\"\"\"\n",
    "\t\tThe accuracy method takes a list of data as input, where each data point consists of input values and \n",
    "\t\tcorresponding labels. The convert parameter determines whether the labels in the data should be converted \n",
    "\t\tto their index in the label vector (for example, the label vector [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \n",
    "\t\twould be converted to 2). \n",
    "\t\tThe method computes the accuracy of the model's predictions on the input data by comparing the \n",
    "\t\tmodel's predicted label with the true label for each data point. The predicted labels are obtained by \n",
    "\t\tusing the model's forward_propagation method on the input values. \n",
    "\t\tThe method returns the percentage of correct predictions.\n",
    "\t\t\"\"\"\n",
    "\t\tresults = [\n",
    "\t\t\t( np.argmax( self.model.optimizer.forward_propagation( x ) ), np.argmax( y ) )\n",
    "\t\t\tfor x, y in tqdm( data, desc=\"Accuracy\", ncols=75, leave=False, position=1 )\n",
    "\t\t] if convert else [\n",
    "\t\t\t( np.argmax( self.model.optimizer.forward_propagation( x ) ), y )\n",
    "\t\t\tfor x, y in tqdm( data, desc=\"Accuracy\", ncols=75, leave=False, position=1 )\n",
    "\t\t]\n",
    "\t\treturn sum( int( x == y ) for ( x, y ) in results )\n",
    "\n",
    "\tdef total_loss(self, data, convert=False):\n",
    "\t\t\"\"\"\n",
    "\t\tThis method calculates the average loss across all the examples in the dataset.\n",
    "\t\tIt also includes a regularization term (lambda) to penalize large weights.\n",
    "\t\t\"\"\"\n",
    "\t\tloss = 0.0\n",
    "\t\tfor x, y in tqdm(data, desc=\"Cost\", ncols=75, leave=False, position=1):\n",
    "\t\t\ta = self.model.optimizer.forward_propagation(x)\n",
    "\t\t\ty = vectorized_result(y) if convert else y\n",
    "\t\t\tloss += self.model.loss.func(a, y)/len(data)\n",
    "\n",
    "\t\tloss += 0.5*( self.model.optimizer.lmbda / len( data ) ) * \\\n",
    "\t\t\tsum( np.linalg.norm(w)**2 for w in self.model.weights )\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "\t\"\"\"\n",
    "\tReturns a trained model.\n",
    "\t\"\"\"\n",
    "\twith open(trained_model_file, \"r\") as f:\n",
    "\t\tdata = json.load(f)\n",
    "\n",
    "\toptimizer  = getattr(sys.modules[__name__], data[\"optimizer\"])\n",
    "\tloss       = getattr(sys.modules[__name__], data[\"loss\"])\n",
    "\tactivation = getattr(sys.modules[__name__], data[\"activation\"])\n",
    "\tmodel      = MLP( \n",
    "\t\tdata[\"layers\"],\n",
    "\t\toptimizer     = optimizer,\n",
    "\t\tloss          = loss,\n",
    "\t\tactivation    = activation,\n",
    "\t\tlearning_rate = data[\"learning_rate\"],\n",
    "\t\tlmbda         = data[\"lmbda\"],\n",
    "\t)\n",
    "\n",
    "\tmodel.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "\tmodel.biases  = [np.array(b) for b in data[\"biases\"]]\n",
    "\treturn model\n",
    "\n",
    "def model_accuracy():\n",
    "\t\"\"\"\n",
    "\tReturns the trained model accuracy.\n",
    "\t\"\"\"\n",
    "\twith open(trained_model_file, \"r\") as f:\n",
    "\t\tdata = json.load(f)\n",
    "\treturn data[\"accuracy\"]\n",
    "\n",
    "def vectorized_result(j):\n",
    "\te = np.zeros((10, 1))\n",
    "\te[j] = 1.0\n",
    "\treturn e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABYsAAADOCAYAAAB/211iAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2lklEQVR4nO3dd3RU1fr/8ScJIbRA6M2EEpUqiFxKQMBCUVRArgiIXoqACPgVBb2KQECUKkUFQUXKVYpKF+9VNAhSQnUBUq8ivQUQJEAgQM7vD3/Jde9zyMycOZOZybxfa7mWnzOn7AkPM2d2hmeHGYZhCAAAAAAAAAAgpIX7ewAAAAAAAAAAAP9jshgAAAAAAAAAwGQxAAAAAAAAAIDJYgAAAAAAAACAMFkMAAAAAAAAABAmiwEAAAAAAAAAwmQxAAAAAAAAAEBE8tg9MCMjQ06cOCHR0dESFhbm5JjgR4ZhSGpqqpQrV07Cw53/XQJ1kztRN7DDl3VDzeRe1A3soG7gKe5tYAd1AzuoG9hB3cAOd+vG9mTxiRMnJDY21u7hCHBHjx6V2267zfHzUje5G3UDO3xRN9RM7kfdwA7qBp7i3gZ2UDewg7qBHdQN7HBVN7Yni6Ojo7MuULhwYbunQYC5ePGixMbGZv35Oo26yZ2oG9jhy7qhZnIv6gZ2UDfwFPc2sIO6gR3UDeygbmCHu3Vje7I482vohQsXpnByIV/9MwPqJnejbmCHL+qGmsn9qBvYQd3AU9zbwA7qBnZQN7CDuoEdruqGBe4AAAAAAAAAAEwWAwAAAAAAAACYLAYAAAAAAAAACJPFAAAAAAAAAADxYoE7AAAAhLaTJ0+atnXs2FHJW7duVfK8efOU3K5dO8fHBQAAAMAevlkMAAAAAAAAAGCyGAAAAAAAAADAZDEAAAAAAAAAQJgsBgAAAAAAAAAIC9wBAADATUuXLlXyM888Y9rn8uXLSu7SpYuSy5Yt6/i4AAAAADiDbxYDAAAAAAAAAJgsBgAAAAAAAAAwWQwAAAAAAAAAEHoWAwAA4P+7efOmknft2qXk7t27K/nq1aumc/To0UPJH3zwgZLz5s3rzRARIvS60fXt2zeHRgJ3LF68WMlz5sxR8vLly03H9O7dW8kxMTHZXqN69eqmbV27dnVzhAAAwF18sxgAAAAAAAAAwGQxAAAAAAAAAIDJYgAAAAAAAACA5NKexRcvXlRyzZo1Tfv89NNPSi5RooTj49D7+OXLl8/xayCwHT582LRt4sSJSn7//feVbBiGkuvUqWM6h94zUu/bFxER4dE4EfjOnj2r5BEjRih5ypQpSv7b3/5mOke1atWUPHnyZCUXK1bMixHCn37++WfTtoYNGyr5hRdeUPKYMWN8OiYEp48//ljJrvrCDhkyxLTtzTffdHRMyH32799v2la1alWPzpGUlGTaNmrUKCVXqVLFs4HBtn79+ik5JSVFyWFhYaZjZsyY4fV16Vmcu506dcrltqFDhyq5ZcuWSv7HP/5hOkeRIkUcGB2gunDhgpIbNGhg2ue3335T8pYtW5R89913Oz0swBa+WQwAAAAAAAAAYLIYAAAAAAAAAMBkMQAAAAAAAABAcmnP4hs3big5LS3NtM/nn3+uZL3Plh1//PGHklu1aqXksWPHmo5p1qyZ19eF/+j9sfW+RHq/NhGR8+fPZ3vO6OhoJW/fvt20z4svvqjkNWvWKHnhwoXZXgOBZfPmzUoePny4aZ8NGzYoOTU1Vcnh4erv/vS+7FbbChcurGS97zHs0V8Xli5dqmS9l16ZMmW8vqbef1rE3Ddf7w351ltvKTlPnlx5S4BsWPW6fumll7I9pn79+kpOTEx0dEzInfQexZ72J7ayePFil/ssWrTI6+vA/NlKxNwfWv8clFN++eUXJd9xxx1+GQecod8TP/jgg6Z9Ll++nO05fv31VyUfOXLEtE+nTp2UrN8Tx8fHK1m/zwastGnTRsn665OVzp07K1n/vJY/f37vBwbbZs+ereS5c+cq2Wr9BF2tWrWUrPfa19ejiomJcX+APsSrHgAAAAAAAACAyWIAAAAAAAAAAJPFAAAAAAAAAABhshgAAAAAAAAAILl0gbuiRYsquV69eqZ9tmzZ4vh1N27cqGS9QX/r1q1Nxxw+fFjJJUqUcHxccM6ZM2eUrC+6oC/gcv/995vOUbduXSXrDc2LFSum5OTkZNM5Xn31VSXr9XzlyhUlFyhQwHQO5JxLly4p+fXXX1fytGnTlGwYhukceqN7vfb0Ojp37pzpHPrCiPp158yZo+Q9e/aYzhEbG2vaBpW+8EH//v2VrP9Z6QvP+Yq+uOYPP/yg5BYtWuTIOOA/+ntDly5dTPtcu3ZNycWLF1fyt99+q+SIiAiHRofcxBcL2sF39AVRV69ereRPP/3UdMyCBQt8OSS31axZU8lPPvmkkj/55BMl582b1+djgvv0xer0+1urxez0xej0BajatWunZP0zuYj1/MBf/fvf/1byQw89lO3+CE09evRQ8rp161weU61aNSV/9913SmZBO+fon6nT0tKU/PXXXyt53rx5pnMsW7ZMyYUKFVKyOwuVp6SkKHngwIFKnjlzppKtFqD2B75ZDAAAAAAAAABgshgAAAAAAAAAwGQxAAAAAAAAAEByac/iY8eOKfmbb74x7aP3hc0Jeo8UEZEbN27k+Djgng0bNpi2tWnTRskPP/ywkvUeb3pPLRGRPHk8+2un990SEbl+/bqSO3bsqOTRo0creeTIkR5dE/bp/YlFzP3K169fn+057rvvPtO26dOnK/mOO+7I9hxWrzeTJ09W8sGDB5VcpEgRJVv1ToZrhw4dyvZxvXdeenq6aR9Peyo+++yzpm16Dz+dVV9r5G76e8GuXbtM+5QtW1bJ7733npL11wlAxNyjePDgwX4aCex44YUXlKz3+Q0LC/P6GhMnTjRt0/vA6vfZBw4ccHle/Z5Y7zm5b98+Ja9du9Z0jnz58rm8Dnxjx44dStZ7FFv92eh/hnfddVe218jIyHA5Dv2e97fffnN5DHxD/zstYl7fQ/98rN+7+IpeF3rPW91zzz1n2qbfV0VGRno/MFh+9n3llVeUrK/Xo7v99ttN2/Se/c2bN1dy6dKl3R1ilvBw9Tu7+nudvq6ZiEiFChU8vo63+GYxAAAAAAAAAIDJYgAAAAAAAAAAk8UAAAAAAAAAAMklPYv1PkSTJk3y00gQTC5evKjkhQsXKtmqV5Xek7hmzZqOj8sdd955Z7aPf/zxx0qmZ3HOseqvp/cojoqKUvLevXuVXL58edM5PO11bdVzW+9RrGvVqpWS4+LiPLom3LN7924l37x50+tz1qlTx7StaNGiSj5//ryS586dq+ROnTp5PQ4EFr0v3vjx410eo/fX+/vf/+7omBD89P7EIiJVq1Z1/Drt27dX8uLFix2/RqgaNGiQkvUexe6sWdCkSRMlDxkyRMmNGjVScsGCBV2ec8uWLUrW71+t+h67Guu2bduU3K1bN9M+c+bMUbJ+nwb/6du3r2mbqx7Fx48fV3Lnzp1dXkev13/84x9ujA6+oP99FBHp16+fkt9++20l62tWOcFqLqBevXpK1u+tx4wZo+SXX37ZdA5PP9PB2tWrV5X82muvmfbRexTr624MGDDA5Tk8XUfGyp49e7J9/Nq1a0o+e/asaR96FgMAAAAAAAAA/ILJYgAAAAAAAAAAk8UAAAAAAAAAgCDtWaz3KNb7bk2ePNnlOV555RUnhyQiIvfff7+S9Z4of/zxh+PXhH3bt29Xcs+ePZU8evRo0zH+6FGs17uIyKpVq7I9pkyZMr4aDlxYvny5y3369OmjZCd6EK1Zs0bJrVu3dnlM/fr1lfzuu+96PQ6YueqnqPdLFxF55plnPLpG/vz5Tdv0vluvvvqqkvV+WBcuXDCdIyYmxqNxwL+uX7+u5Ndff13J+vtJQkKC6RzDhg1zfmAIah988IGSk5KSPD6H3n/4wQcfNO2j96SkR7E9+nvOp59+atpH72ceFhaW7Tn1NQ1ERKZOnarkypUruzvEW9I/O40dO1bJVu91eu9SV8/lyy+/NG0bNWqUkp14LnBGcnKyaduVK1eUvGzZMiXrn+nS0tJM5yhdurSS9XouVKiQR+OEfYcPH1ay3kNdxPy6ZvVn6q309HQlW/W61nsUN2/eXMndu3dXMv2JnaP3KO7QoYOSv/76a9MxBQoUUPKPP/6oZF/M7VjN+ek9/nXFihVTcvXq1R0dk118sxgAAAAAAAAAwGQxAAAAAAAAAIDJYgAAAAAAAACAMFkMAAAAAAAAAJAgXeBuxIgRSnZnQTvd448/7tBo/idv3rxKHjJkiJKtFtVbu3atkvVG3fAdveG8vmiG/ucnIlKiRAklP/vss46Pa9++fUoeOnSoaZ9FixYpWR/7tGnTHB8X3FO8eHGX+zz22GMen/fGjRtK1hen0etE319EpF69ekqeMWOGklnMwxn630dXi+34in5dPW/atEnJM2fONJ3j5Zdfdn5gcITV33F9YRV9ARj977jVIjKATl94zg59AbEqVaqY9tEXvRs8eLCS3VnwTr9OKDp27JiS9dcFd+gLO02cONG0j75AmC9EREQoeeTIkaZ9Pv/8cyUfOHDA4+t89NFHSh4zZozH54A9tWrVUnLBggWVbLXAXY0aNZSsL5Dm6hoi5tcTFjXMOWfOnFFytWrVlKwvZCYiEhkZqWT9770T9MXrtmzZ4vIYfV6qZMmSjo4J/zNlyhQl6wva6a8LIiIrVqxQshOLyuv091yr+xv9flxfsFH/XG+1mKs/8M1iAAAAAAAAAACTxQAAAAAAAAAAJosBAAAAAAAAABIEPYt//vln07b333/fo3MsXbrUtC0+Pt6jc1j1Bty4caOSd+zYoWS9r4qVnTt3KpmexTmnUaNGSh4/frySBw4caDrmxRdfVPLevXuV/M4777i87uXLl5Xcp08fJes9mKxqT9ezZ08lJyQkuDwGvtG4cWOX+yQmJir53nvvVXJGRobpmN69eyv5s88+y/Yao0ePNm3r27evkulR7BvPPPOMkq16LAaiL7/80rStffv2Sq5YsWIOjQaunD171rRt3rx5StZ7or399ttKrlq1qvMDQ9DZv3+/ku3Uhf5a4U6PYm/p1wxV169fV/Lw4cO9Pme3bt2UnBP9ie1avny5kq36VrqifwagZ3HO0e9F9TV+rOpZ71EcHq5+/23VqlVK1j/ziZjXrkHOadOmjZL1HsV169Y1HTN37lwl33nnnV6P49KlS0r+9NNPPT5HgQIFvB4H3LN9+3Yl62uxWK1J5qpHcXp6upKt5l0uXryo5LFjxyp52bJlSrbqua3Xib5umVVf9UDAN4sBAAAAAAAAAEwWAwAAAAAAAACYLAYAAAAAAAAASAD2LNZ7dQ4YMMC0z4ULFzw654YNG0zbFi9erORvv/3Wo3GJiJw5c8ajcViZOHGikvW+TBEREV5fA+7R+7la9Y7R+8ZOnTpVydHR0UquX7++6RxPP/20ks+fP5/tuJ544gnTto4dOyq5bdu22Z4DOUfvn2Rl/fr1Sk5JSVHya6+9ZjpG70WqmzBhgpL/7//+z7SP3tMNvpGamurR/v369TNte/XVV5Ws9551p870XmyubN682bStevXqSn7++eeVrNcdfEfvo9a5c2eXx5QvX17JTz75pMtj9LrRe6/dvHlTyXqffRGRZs2aKbl27dour4ucofcnFnGmd/WiRYu8PkdSUpKS9Xt1nd4XWcQ3vZED3ZEjR5Q8e/Zsr89p1eM1UEVGRvp7CHDQb7/95vEx48aNU3LTpk2dGg4coH/O+emnn7Ld/7nnnjNt87RHsX7fLGKez5k0aZKS9TVlrBQtWlTJZcqU8WhcsE+/R9CdO3fOtO3ZZ5/N9pgtW7Yoeffu3aZ9PP0MVqRIEdO2H3/8Uck1a9bM9hyBgpkDAAAAAAAAAACTxQAAAAAAAAAAJosBAAAAAAAAABKAPYuPHj2q5B9++MHrc+p9jAJJWlqav4eAW7j33ntN26ZMmaLk5s2bK3nEiBEeX0fvfaT32L7jjjtMx9B7NnBZ9bp+6aWXlKz3yIqLi3N53tKlSytZ7+XYsGFDd4cIH5s/f75H+1++fNnlNjs9i51w7do1Ja9ZsyZHrgszvRf2nj17TPvkyaPe1s2YMUPJ+uuI1XoM//rXv5Tcv39/j8YpIlKoUCElr1u3TslWr5PwDb1H8eDBgz0+R/v27ZVs1SvYU1a9k131I4S1s2fPKtmqV6crv/76q5ILFizo1Zj8yc7zh/+8/fbbStbfg9zRqlUrp4YDH/juu++UfP369Wz3f+qpp1yeU79/0eeQrNZ6eeONN1ye15Vt27YpWb+vgu8UK1ZMyXov7GnTppmOyanPS39lteZasPQo1jHjBAAAAAAAAABgshgAAAAAAAAAwGQxAAAAAAAAAECYLAYAAAAAAAAASAAucBcbG6tkq2bQu3btyqnhZKtNmzZKrlSpkpL1BYr0hWYQWPQFMU6cOGHa59VXX/X6OgMHDlSy3mw/JibG62sgsHTu3FnJ+gJ3OqvFEj777DMls6Bd4KpSpYqS8+fPr+QrV654fE5fLNhj55xWC6IhZ+jvFWfOnDHto9+HtGjRQsnnzp1T8qBBg0znmDNnjt0hZrl06ZKS9cVhP/roI6+vAWv6wnFVq1b1+Bz6gnaLFi3yakwizoxr3759StZfa0PVkCFDlOzOgj76QkHBfO+Znp6uZDsLGlktjATfuHr1qpL115eSJUsquU+fPqZzjBw5UskLFizI9nHknLS0NNM2fRFDV7766ivTNn0hz88//1zJ+n3H9u3bPbqmlU6dOpm2ubMoOXzjk08+UfKYMWOUvHz5ctMxCQkJSr733nuzvYZ+n3Gr8/6VPifoxEKKgYJvFgMAAAAAAAAAmCwGAAAAAAAAADBZDAAAAAAAAACQAOxZHB6uzl//61//Mu2j9+48duyYkn/88Ucl9+rVy3SOI0eOKFnvnfbiiy+6HGtkZKSSIyIilHzx4kUlu9Oz+MaNG9meE76j11r37t1dHhMVFaXk4sWLK9mq77HevzaY+8TB7MCBA6ZtLVu29OgcVv2wmjRpYntMyFk9evRQst7nd9iwYUrWXzdEzD2p9fcovZ//2LFjTecoWLCgkr///nslX7t2TclW7zfPP/+8kocOHWraB76h93XU+/PlyWO+hdNr7z//+Y+S9d6PVu9Ret/jWrVqKbljx45K3rFjh+kcej3qfZBff/31bK8J933wwQdKTkpK8vgcOdGjePDgwR6fQx8XPYqt6X/m7vTs/f3335V84cIFJes9jQPZiBEjPNq/SJEipm2PP/64U8OBC3ofWL23rP4532odD924ceOUTM9i/9FfW0Ss+8BmR/+sbEVfD0S/32nVqpXpGP2+V1euXDklT58+3bSPPleFnKN/Nlq6dKnj13jllVdM2/Q1XvQ5wPHjxys5N83fUe0AAAAAAAAAACaLAQAAAAAAAABMFgMAAAAAAAAAJAB7Fuvuvvtut7blFlu2bFHyvffe66eR5H7JyclKHjRokMtjKleurOStW7cqWe8xGR8fbzpH8+bNlbx7924lly1b1uU4EDgOHjyo5MaNG5v20XsBtm7dWskbNmxQsv46ICKyadMmJfPaEDx69uyZbXaCVW823WOPPabkr7/+WsmFCxc2HTNq1CglFypUyMboYMfevXuVfP78eSVb9Sw+fPiwkhMTE5Ws98+28sknnyhZv+f69ddflaz3m4Tv6H2ARUT69evn0Tn0PsAizvQo1nsnezouEXNfS3oUu0fvg2+nX+t7772n5MmTJ3szJJ/R16UREfniiy+U7Kpn85o1a0zbSpYs6d3AYOnmzZumbfo9rt7/s3bt2ko+deqU8wODz1jdJ5YqVUrJKSkpHp+3TZs2StZf5+666y4lV6tWzXQOV/cr+lyA1X0xche9Fq36VOvvKcOHD1fy7bff7vi4AgXfLAYAAAAAAAAAMFkMAAAAAAAAAGCyGAAAAAAAAAAgQdCzONQkJCT4ewghY8WKFUo+d+6ckrt06WI65p133lFyTExMttf49ttvTduaNm2q5EOHDimZnsWBLT09Xcmvvfaaks+cOWM6pnfv3kqeMmWKkl9//XUlT5gwwXQOvV7pWQxP9erVS8l6z2K9t7aIuUdq3bp1HR/XgQMHlFy8eHHTPq5ea3MjvR+67saNG6ZtM2bM8Pq6Y8aMUfLKlSs9PkexYsWUvGzZMiVXqlTJ84GFIP3vX9WqVT0+h96jWO9D7g69H3FSUpJpn8WLF3s0jgcffNC0Dz2K7XnmmWeUbKdn8fvvv6/krl27KrlOnTqeD8wB+log48eP9/gcej9J6izn7Ny507Tt5MmTStZfk2rUqKFkehYHlyJFipi2HTlyRMn6mgxXrlxRstXrjd7bOiIiQsn6egpWtadbsmSJkh955BGXxyB36dixo5L1WhQx99zu37+/T8cUSPhmMQAAAAAAAACAyWIAAAAAAAAAAJPFAAAAAAAAAABhshgAAAAAAAAAICxwF3D0RutPPPGEn0aS++nN9vUFeaZNm2Y6plChQh5do3r16qZt+kJNb775ppL1hcz0Bv7wryFDhih54cKFSn7ggQdMx7z33ntK1v9M9UXyrBa4A7yVP39+fw/BUlxcnJIzMjL8NJLAov9ccoqnC9rVqlXLtE2/l2FBO3vsLGg3depUJfft29flMfpCeoMHD1ayq8XrnBoH7ImPj1fyK6+8ouRx48Z5fM577rlHyY0aNTLt8/TTT2ebo6OjPb5uSkqKkvXFpq0WVzQMQ8n6Qlj6osJ58+b1eFxwz/Xr15U8cOBAl8c8+uijXl+3dOnSXp8DvqP/natdu7bX57x586aS9UU63bmX1BfSy5OHqbHcRq8D/R533bp1Ls/xxRdfKNnOe1uw4pvFAAAAAAAAAAAmiwEAAAAAAAAATBYDAAAAAAAAAISexTlK76klIhIWFqZkvdcTfOfzzz9X8ogRI5TsaX9iK3ofZBGRmjVrKvnbb79Vcnp6upIDtc9oqEhLS1Oy3lO6TJkySp45c6bpHHr/PF3BggVdjkPv/zhq1Cglh4fzuz9k791331Wy1XuSTu8f6Quu/n6EqgoVKihZ78l4+vRpn1xXr4vKlSsrWe9B+eyzz5rOERUV5fzAQoB+T2iHVU/Xv+rXr5/X12jfvr1pm/6eVKVKFa+vA3v0dRA+/vhj0z4XLlzw6JwbN250ue2tt95Ssv6a5c7noIMHDyr54sWL2e4vYn4P0dcCadGihekY+Ib+OXb16tVen/O///2vy33Wr1/v9XUQXPRe+3rPYiv6OgBFixZ1dEwIPIcOHVLyI488ku3+o0ePNm1r2rSpk0MKKswuAAAAAAAAAACYLAYAAAAAAAAAMFkMAAAAAAAAABB6FucoJ3rRwXd80RvYqj9bRkaG49eB7+g9pfUeWdOnT1dybGysx9fo3bu3y31at26tZF5P4Cm9ZlzlW21DzihZsqSSd+3apeQXXnjBdMyCBQuyPafeX7hIkSKmfR588EElJyQkKDkmJibba8C/Fi9enG22Q++Zr9eICD2KA4n+d/Tw4cOmffS+xgsXLlSynX71p06dyja707PYjpYtWyr5n//8p9fnhO/kyaNOP+TLl0/Jv//+u5LHjRtnOkdcXJyS6T0beoYNG5bt41avLfPnz1dydHS0o2OCfx04cMC0rX79+krW34f0z+1OrOuQm/DNYgAAAAAAAAAAk8UAAAAAAAAAACaLAQAAAAAAAABCz+KA06hRI38PIWTo/a70Pka1atUyHVOmTJlsz3n06FElz50717TPhg0bsj1neDi/wwkkVj09/2rNmjVKbtOmjWmfLVu2KFnvK7po0SIlW9VA586dlUwvWeQEvY/lQw895KeRoHjx4kqeN2+eaR+rbYAr7du3V/KoUaOUTD/i4FaoUCHTtilTpij5nXfeUfLzzz+v5J9//tl0Dr23rFVvZE/p91zdunVT8lNPPWU6pmbNml5fFzlH71k8Z84cJeufnazqqk+fPkq2qnHkLhcuXFDy6tWrs93/pZdeMm2rXbu2gyOCv504cULJen9iEXPd6D3Sk5OTlVywYEFnBpdLMCsFAAAAAAAAAGCyGAAAAAAAAADAZDEAAAAAAAAAQJgsBgAAAAAAAAAIC9z5XcWKFZUcGxvrn4GEoI4dOyp59OjRSm7evLlPrqsvFLNy5UolR0VF+eS6sEdfdPJvf/ubkvWFEfXsjttuu03JY8aMMe3Trl07j88L/FWlSpWyfTwiIsK0rVWrVr4aDgDNvn37lJyUlOT4Nfr27ev4ORH89EV/Zs2a5fKYtLQ0JZ88eVLJBw8eVLKr9yARkejoaCWXLFnS5TEILlevXlXy22+/ne3+Vp/Hhg4d6uiYEPgOHTqkZH2BTX3xuhdeeMHXQ4Kf6Z+Xz58/b9pHf29btWqVksuVK+f8wHIRvlkMAAAAAAAAAGCyGAAAAAAAAADAZDEAAAAAAAAAQOhZnKMSEhJM2/75z38qOTyc+fuc0r17dyUnJycrefXq1aZjypQpo+Snn34622uUKFHCtG3AgAFKzps3b7bngH/pPaQHDRqk5GnTpil5zZo1pnM0bdpUyZ07d8426z37ACe89957Si5fvryS3333XdMxHTp08OmYAPyPvqaBnoFAkj9/fiVXrlw524zcT79n/vDDD037vPTSS0rW1+u58847lTxp0iTTOcqWLWt3iAhSu3fvzvbxxMREJVeoUMGXw0EOuHbtmpLHjx+v5I8//tjlOXr27Knkhg0bej+wEMLMJAAAAAAAAACAyWIAAAAAAAAAAJPFAAAAAAAAAAChZ7FPFS5cWMnr16/300hg5fbbb1fyqlWr/DQSBBO9hys9XRGs9J75egYAAHBXRESEknv16mXax2ob4EqXLl2yzch9zp49q+QxY8YoOT09XclW64NNmDDB+YGFEL5ZDAAAAAAAAABgshgAAAAAAAAAwGQxAAAAAAAAAEDoWQwAAAAAAAAgAJQvX17Jly5d8tNIQhffLAYAAAAAAAAAMFkMAAAAAAAAAGCyGAAAAAAAAAAgTBYDAAAAAAAAAITJYgAAAAAAAACAMFkMAAAAAAAAABAmiwEAAAAAAAAAIpLH7oGGYYiIyMWLFx0bDPwv888z88/XadRN7kTdwA5f1g01k3tRN7CDuoGnuLeBHdQN7KBuYAd1AzvcrRvbk8WpqakiIhIbG2v3FAhgqampUqRIEZ+cV4S6ya2oG9jhi7qhZnI/6gZ2UDfwFPc2sIO6gR3UDeygbmCHq7oJM2z+GiIjI0NOnDghhmFIXFycHD16VAoXLmx7oPjTxYsXJTY21m8/T8MwJDU1VcqVKyfh4c53KaFufIO6gR25uW4yayY6OlpSU1P9+jxzm1CoG15rnEfdwA5/1g33NsGLuoGncvN7lAh14yvUDewIlrqx/c3i8PBwue2227K+wly4cGEKx0H+/Hn64rdSmagb36JuYEdurJvMmhERCQsLExHqxmm5uW54rfEd6gZ2+Otnyr1NcKNu4Knc+B4lQt34GnUDOwK9bljgDgAAAAAAAADAZDEAAAAAAAAAwIHJ4qioKElMTJSoqCgnxhPyQuXnGSrPM6eEys8zVJ5nTgmVn2eoPM+cEgo/z1B4jjktFH6mofAcc1oo/ExD4TnmtFD4mYbCc8xJofLzDJXnmVNC5ecZKs8zpwTLz9P2AncAAAAAAAAAgNyDNhQAAAAAAAAAACaLAQAAAAAAAABMFgMAAAAAAAAAhMliAAAAAAAAAIAwWQwAAAAAAAAAEC8ni6dOnSoVK1aUfPnySYMGDWTz5s1OjSvXGz16tNSrV0+io6OlVKlS0q5dO9m/f7+yz9WrV6Vfv35SvHhxKVSokPz973+X06dP+2nEzqFu7KNuqBtPhXLNiFA3dlE31I0d1A11Ywd1Q93YQd1QN3aEct1QM/ZRN9SNp3JFzRg2LViwwMibN68xc+ZMY/fu3UavXr2MmJgY4/Tp03ZPGVJatWplzJo1y9i1a5exfft2o3Xr1kZcXJxx6dKlrH369OljxMbGGklJScbWrVuNhg0bGo0aNfLjqL1H3XiHuqFuPBWqNWMY1I03qBvqxg7qhrqxg7qhbuygbqgbO0K1bqgZ71A31I2nckPN2J4srl+/vtGvX7+sfPPmTaNcuXLG6NGjHRlYqElJSTFExFizZo1hGIZx4cIFIzIy0vjyyy+z9tm7d68hIkZycrK/huk16sZZ1A1146lQqRnDoG6cRN1QN3ZQN9SNHdQNdWMHdUPd2BEqdUPNOIu6oW48FYw1Y6sNRXp6umzbtk2aN2+etS08PFyaN28uycnJtr7hHOr++OMPEREpVqyYiIhs27ZNrl+/rvyMq1atKnFxcUH7M6ZunEfdBOdz8qdQqBkR6sZp1A11Ywd1Q93YQd1QN3ZQN9SNHaFQN9SM86ib4HxO/hSMNWNrsvjs2bNy8+ZNKV26tLK9dOnScurUKUcGFkoyMjJkwIAB0rhxY6lZs6aIiJw6dUry5s0rMTExyr7B/DOmbpxF3QTvc/KXUKkZEerGSdRN8D8vf6Bugv95+QN1E/zPyx+om+B/Xv4QKnVDzTiLugne5+QvwVozefw9AIj069dPdu3aJevWrfP3UBBEqBt4ipqBHdQN7KBuYAd1AzuoG9hB3cAO6gaeCtaasfXN4hIlSkhERIRppb7Tp09LmTJlHBlYqOjfv7+sWLFCfvjhB7ntttuytpcpU0bS09PlwoULyv7B/DOmbpxD3QT3c/KHUKoZEerGKdTNn4L9eeU06uZPwf68chp186dgf145jbr5U7A/r5wWSnVDzTiHugnu5+QPwVwztiaL8+bNK3Xr1pWkpKSsbRkZGZKUlCQJCQmODS43MwxD+vfvL0uWLJFVq1ZJpUqVlMfr1q0rkZGRys94//79cuTIkaD9GVM33qNu/kTduC8Ua0aEuvEWdUPd2EHdUDd2UDfUjR3UDXVjRyjWDTXjPermT9SN+3JFzdhdGW/BggVGVFSUMXv2bGPPnj1G7969jZiYGOPUqVPeL7sXAp5//nmjSJEixurVq42TJ09m/XflypWsffr06WPExcUZq1atMrZu3WokJCQYCQkJfhy196gb71A31I2nQrVmDIO68QZ1Q93YQd1QN3ZQN9SNHdQNdWNHqNYNNeMd6oa68VRuqBnbk8WGYRjvv/++ERcXZ+TNm9eoX7++sXHjRqfGleuJiOV/s2bNytonLS3N6Nu3r1G0aFGjQIECxuOPP26cPHnSf4N2CHVjH3VD3XgqlGvGMKgbu6gb6sYO6oa6sYO6oW7soG6oGztCuW6oGfuoG+rGU7mhZsIMwzCc+Y4yAAAAAAAAACBY2epZDAAAAAAAAADIXZgsBgAAAAAAAAAwWQwAAAAAAAAAYLIYAAAAAAAAACBMFgMAAAAAAAAAhMliAAAAAAAAAIAwWQwAAAAAAAAAECaLAQAAAAAAAADCZDEAAAAAAAAAQJgsBgAAAAAAAAAIk8UAAAAAAAAAAGGyGAAAAAAAAAAgTBYDAAAAAAAAAITJYgAAAAAAAACAMFkMAAAAAAAAABAmiwEAAAAAAAAAwmQxAAAAAAAAAECYLAYAAAAAAAAACJPFAAAAAAAAAABhshgAAAAAAAAAIEwWAwAAAAAAAACEyWIAAAAAAAAAgDBZDAAAAAAAAAAQJosBAAAAAAAAAMJkMQAAAAAAAABAmCwGAAAAAAAAAAiTxQAAAAAAAAAACfDJ4rCwMOW/8PBwiYmJkSZNmsiMGTPEMAy/jm/27NkSFhYmw4cP98n5z507J6VKlZKwsDC5/fbbfXINOG/Lli3y5JNPSrly5SQyMjKrZmfNmuX3mkXgom7gDd4v4IkzZ87IoEGDpEqVKpI/f34pVqyY3HPPPfLKK6/4e2gIUGlpaTJs2DC58847JV++fFKuXDnp0aOHHD9+3N9DQwCjbmAH98TwBvfEcNfevXulS5cuUrZsWYmKipKKFStK//795ezZs/4eWkDI4+8BuKNr164iInLz5k05cOCArF+/XtatWydJSUkyf/58P4/OdwYOHEihBplFixZJx44d5ebNm3LPPfdIkyZN5MyZM7J27VpZt26dfP/99zJ37lx/DxMBhrqBt3i/gLu2bdsmrVq1knPnzkmNGjWkbdu2cvHiRdmzZ49MmjRJxo8f7+8hIsBcvXpVHnjgAdm4caOULVtW2rZtK4cOHZJZs2bJihUrZOPGjVK5cmV/DxMBhrqBHdwTw1vcE8Mdq1atkscee0yuXLkiVatWlUaNGsmuXbtk6tSpsmzZMklOTpbbbrvN38P0LyOAiYhhNcSVK1caefLkMUTE+Oqrr/wwsj/NmjXLEBEjMTHR8XN///33hogYvXv3NkTEiI+Pd/wacNb169eNUqVKGSJizJ07V3lsz549RrFixQwRMVatWuWnESIQUTfwFu8XcFdKSopRokQJo0CBAsayZctMj2/atMkPo0Kge+ONNwwRMRISEozU1NSs7RMmTDBExGjWrJn/BoeARd3AU9wTw1vcE8Mdly9fNkqXLm2IiDFs2LCs7RkZGcagQYMMETFatmzpxxEGhoBuQ3ErLVq0kGeeeUZERJYuXerfwfhAWlqaPPfcc1K9enUZNGiQv4cDN+3bt09SUlKkSpUq8tRTTymPVatWTZ5++mkR+fOfVgGZqBt4g/cLeCIxMVHOnj0r48ePlzZt2pger1+/vh9GhUCWnp4uU6ZMERGRqVOnSqFChbIee/nll6VWrVqyZs0a2bZtm7+GiABE3cAO7onhDe6J4a7FixfL6dOnpUqVKpKYmJi1PSwsTEaNGiUVK1aUlStXyo4dO/w4Sv8LysliEZE6deqIiMjRo0eztoWFhUnFihUlPT1d3nzzTalatapERUVJu3btsva5cuWKjB49WurUqSOFChWSQoUKScOGDWXOnDm3vNb69eulefPmEh0dLTExMdKqVSvZtGmTz57biBEj5LfffpPp06dLZGSkz64DZ0VFRbm1X/HixX08EgQT6gbe4P0C7kpLS5PPPvtMChYsKN27d/f3cBAk1q9fL3/88YfEx8dn3Xv/1RNPPCEiIl999VVODw0BjLqBHdwTwxvcE8Ndmb+obNq0qYSHq1OikZGR0rhxYxERWbZsWY6PLZAE7WRxamqqiJjfVDIyMqRdu3Yybtw4iY+Pl7Zt20rZsmVFRCQlJUUSEhJk8ODBcurUKWnWrJk0bdpU9u3bJ926dZMXXnjBdJ0VK1bIfffdJ0lJSVK9enV5+OGH5ejRo9K0aVNJTk62HNuhQ4eyFuXz1M6dO2XChAnSvXt3adKkicfHw38qV64s8fHxsn//fpk3b57y2N69e+Wzzz6TokWLyuOPP+6nESIQUTewi/cLeGLr1q2SmpoqderUkfz588t//vMfefnll6Vv374yefJkOXHihL+HiACU+a2ae+65x/LxzO07d+7MsTEh8FE3sIN7YtjFPTE8cfnyZRERKVq0qOXjmb+QCvVvFgfFAnc6wzBkxYoVIiJSq1Yt5bGjR49KVFSU7N+/X8qXL6881r17d9m5c6e8+OKLMnbs2KyJ5tOnT8ujjz4qU6ZMkUceeUQeeughEflzQrpHjx5y48YNmTlzZtY3cQzDkNdff13Gjh3r6PPKyMiQnj17SkxMjIwbN87Rc8P3IiIiZM6cOfLoo49Kly5dZMKECXLHHXdISkqKrF27VqpXry6zZ8+WYsWK+XuoCCDUDezg/QKe2rNnj4iIlCpVStq1a2f6tsTgwYPlk08+kc6dO/tjeAhQR44cERG55SIvmdsPHz6cY2NC4KNuYAf3xLCDe2J4qmTJkiJy6/eggwcPZvt4qAiqbxbfvHlTfvnlF+nRo4ckJydLVFSU5T+lHD16tGmiePv27fLvf/9b6tWrJxMnTlS+kVy6dGn56KOPRERk2rRpWdsXLlwoZ86ckaZNmyrXCQsLk5EjR97yBigyMlKqVKkiVapU8ej5vf/++7JlyxYZP348/7wmSDVu3FjWrFkjlStXlp9++kk+//xz+eGHHyQ8PFxatGjBqs+wRN3AU7xfwFPnz58XEZHly5fLN998I1OnTpWUlBQ5dOiQDBo0SNLS0qRr166yfft2/w4UAeXSpUsiIlKgQAHLxwsWLCgi//sXf4AIdQP7uCeGp7gnhqeaNm0qIiJff/21nD17Vnns+PHj8t1334kI71FBMVmc2dIhT548cuedd8rs2bMlOjpa5s+fL/Hx8aZ9H3vsMdM5Vq5cKSIi7dq1M/UlEZGsHsabN2/O2rZ27VoREenUqZNp/8jIyKx+W7ry5cvLvn37ZN++fW4/xyNHjsiQIUOkWbNm0q1bN7ePQ2CZP3++1K9fX2JjY2XTpk1y6dIl+e9//yvdunWTCRMmyAMPPCDXrl3z9zARYKgbeIL3C9iRkZEhIiI3btyQN998U/r27SslS5aUChUqyPjx46VDhw5y/fp1GT9+vJ9HCgAIVdwTwxPcE8OOli1byj333COXLl2Shx9+WDZv3iyXLl2S5ORkefjhh+XGjRsiIpbzhqEkKNpQdO3aVUT+/MMqXLiw3HXXXdK+fXvLHiOlSpWybI5/6NAhERF544035I033rjlta5evZr1/5n9+ypUqGC5b8WKFd19Ci7169dP0tPTZfr06Y6dEznrl19+ka5du0qpUqVkxYoVWSs/33HHHfLhhx/KiRMnZMWKFTJz5kx5/vnn/TxaBArqBp7i/QJ2ZL62iIjlv8rq3r27fPnll7JmzZqcHBYCXGbdXLlyxfLxzL5/0dHROTYmBD7qBnZwTwxPcU8MO8LCwmTx4sXyyCOPyNatW6VBgwZZj5UuXVqGDx8uQ4YMuWVP41ARFJPFs2fPdnvffPnyWW7P/EbNvffea/o2ciBYsWKFxMTESJ8+fZTtmZPXx48fl/vuu09ERBYsWCBlypTJ6SHChQULFsj169floYceUj6UZ3ryySdlxYoV8uOPP3KDgyzUDTzF+wXsyPzFd4ECBbJ6tf1V5i/AU1JScnJYCHBxcXEiInLs2DHLxzO33+qLFQhN1A3s4J4YnuKeGHZVqFBBtm/fLkuWLJENGzZIWlqa1KhRQ7p06SKLFy8WEZEaNWr4eZT+FRSTxU7I7C/crl07GThwoFvHlC1bVkRu3dja6YbXFy5cuOU3eq5evZr12F+//YzAkXnjW6RIEcvHM7dn9o0ERKgb2MP7BTxVp04dERFJS0uTa9eumf4V1u+//y4iYvkBHaGrdu3aIiLy008/WT6euV1fcBqhjbqBHdwTww7uiWFXnjx5pEOHDtKhQwdl+4YNG0REsn7REKpCpglHixYtRERkyZIlbh/TpEkTERH54osvTI/duHFDFi1a5MzgRMQwDMv/MldijI+Pz9rmZPsLOCfzN5Vbt261fHzLli0i4mz7EgQ/6gae4v0CdsTFxUnt2rXFMAzLD1WZ2zInlQGRPxebKlKkiBw4cMBy8cOFCxeKiFiuF4LQRd3ADu6J4SnuieG0U6dOycKFC6V48eLSvn17fw/Hr0JmsrhBgwbSokULWb9+vfTr108uXrxo2mfHjh3yzTffZOUOHTpI8eLFZfXq1TJnzpys7YZhSGJiohw5csTyWsePH5eqVatK1apVnX8iCFht27YVEZEff/xRpk2bpjy2ceNGmTRpkojILRdGRGiibgDklFdffVVERAYNGiQnT57M2r59+3aZMGGCiIjpn3IitOXNm1f69+8vIn/2hszsNSsiMnHiRNm5c6c0a9ZM6tat668hIgBRN7CDe2IAOWXXrl2mb5sfO3ZM2rZtK6mpqTJhwgTJnz+/n0YXIIwAJiKGJ0MUEaNChQq3fPz06dNGnTp1DBExYmJijPvuu8946qmnjEceecSIjY01RMR48cUXlWOWLl1qREREGCJiNGjQwOjcubNRvXp1IzIy0ujVq5chIkZiYqJyzMGDBz0e+61knis+Pt7rc8H3Bg0alPVnX6NGDaNDhw5G48aNjfDwcENEjN69e/t7iAhA1A2cwPsF3NG1a9es+6DWrVsb999/vxEVFWWIiNGrVy9/Dw8BKC0tzWjQoIEhIkbZsmWNJ598MiuXLFnSOHDggL+HiABE3cAO7onhBO6J4UrXrl2NwoULG/fdd5/RuXNn48EHH8y6Hx46dKi/hxcQwgzDMHJmWtpzYWFhIvLnN3nd3b9ChQpy6NChW+5z9epV+fjjj2XBggWye/duSUtLk9KlS0vlypXl0UcflU6dOmX1N860du1aSUxMlM2bN0tERITUq1dPRo4cKfv375fu3btLYmKiDB8+PGv/Q4cOSaVKlTwa+61knis+Pl5+/fVXr86FnLFkyRKZPn26bNu2Tf744w+Jjo6Wu+++W3r16iWdO3f29/AQoKgbeIv3C7jDMAyZMWOGfPjhh7J3714JCwuTWrVqyXPPPSddu3b19/AQoNLS0mT06NEyb948OXr0qBQrVkweeughGTlypOm+GchE3cAO7onhLe6J4crSpUtl+vTpsmPHDjl37pwULVpUEhISZMCAASHfqzhTQE8WAwAAAAAAAAByRsj0LAYAAAAAAAAA3BqTxQAAAAAAAAAAJosBAAAAAAAAAEwWAwAAAAAAAACEyWIAAAAAAAAAgDBZDAAAAAAAAAAQJosBAAAAAAAAAMJkMQAAAAAAAABAmCwGAAAAAAAAAAiTxQAAAAAAAAAAYbIYAAAAAAAAACBMFgMAAAAAAAAAROT/AUhdt2Wdn6weAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1800x1800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_data, val_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "model = MLP( \n",
    "\tlayers        = [ 784, 30, 10 ],\n",
    "\toptimizer     = SGD,\n",
    "\tloss          = CrossEntropyLoss,\n",
    "\tactivation    = Sigmoid,\n",
    "\tlearning_rate = 0.1,\n",
    "\tlmbda         = 5.0,\n",
    ")\n",
    "\n",
    "monitor = Monitor(\n",
    "\ttraining   = True,\n",
    "\tvalidation = True,\n",
    "\tsave       = False,\n",
    ")\n",
    "\n",
    "if 0:\n",
    "\tmodel.fit(\n",
    "\t\ttrain_data     = train_data[:10000],\n",
    "\t\tval_data       = val_data[:1000],\n",
    "\t\tepochs         = 5,\n",
    "\t\tbatch_size     = 32,\n",
    "\t\tcontinue_train = False,\n",
    "\t\tmonitor        = monitor\n",
    "\t)\n",
    "\tevaluation_cost, evaluation_accuracy, \\\n",
    "\ttraining_cost, training_accuracy = monitor.history()\n",
    "\n",
    "model   = load_model()\n",
    "n_preds = 10\n",
    "np.random.shuffle(val_data)\n",
    "\n",
    "fig, axes = plt.subplots(1,n_preds, figsize=(18,18))\n",
    "\n",
    "for i,ax in enumerate(axes.flat):\n",
    "  ax.imshow( np.reshape(val_data[i][0], (28,28)), cmap=\"Greys\" )\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  prediciton = 'Pred: ' + str(model.predict( val_data[i][0] )) if i==0 else str(model.predict( val_data[i][0] ))\n",
    "  ax.set_xlabel( prediciton, labelpad=18, size=15 )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ab41e39c88aca726516e64836adb3baadd455e52bdb5f7c2d51dfedaf114bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
